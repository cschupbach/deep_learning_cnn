{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "## Feature Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Conv2D, Conv3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv1d, Conv2d, Conv3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes input image $x_\\text{in}$ and applies a kernel function $\\Omega$, such that,\n",
    "\n",
    "$$x_\\text{out} = \\Omega(x_\\text{in})\\tag{1}$$\n",
    "\n",
    "where the output image $x_\\text{out}$ is the result of convolution using kernel function $\\Omega$ and the input image $x_\\text{in}$. The dimensions of $x_\\text{out} \\in \\mathbb{R}^{W_\\text{out} \\times H_\\text{out} \\times D_\\text{out}}$ are dependent on the dimensions of $x_\\text{in} \\in \\mathbb{R}^{W_\\text{in} \\times H_\\text{in} \\times D_\\text{in}}$ and the hyperparameters of the kernel function $\\Omega$: the number of kernels $K$, kernel size $F$, the stride $S$, and the amount of padding $P$.\n",
    "\n",
    "$$x_\\text{in} \\in \\mathbb{R}^{W_\\text{in} \\times H_\\text{in} \\times D_\\text{in}} \\quad\\overset{\\Omega}{\\longrightarrow}\\quad x_\\text{out} \\in \\mathbb{R}^{W_\\text{out} \\times H_\\text{out} \\times D_\\text{out}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$W_\\text{out} = \\frac{W_\\text{in} - F + 2P}{S} + 1\\tag{2}$$\n",
    "\n",
    "$$H_\\text{out} = \\frac{H_\\text{in} - F + 2P}{S} + 1\\tag{3}$$\n",
    "\n",
    "$$D_\\text{out} = K\\tag{4}$$\n",
    "\n",
    "For example, let $x_\\text{in} \\in \\mathbb{R}^{5 \\times 5 \\times 1}$ and the kernel filter $\\omega \\in \\mathbb{R}^{3 \\times 3}$. We compute the dimensions of the $x_\\text{out}$ resuting from convolution of $x_\\text{in}$ and $\\omega$ using stride $S=1$ and no padding $P=0$ as follows:\n",
    "\n",
    "$$W_\\text{out} = \\frac{W_\\text{in} - F + 2P}{S} + 1 = \\frac{5 - 3 + 2(0)}{1} + 1 = 3$$\n",
    "\n",
    "$$H_\\text{out} = \\frac{H_\\text{in} - F + 2P}{S} + 1 = \\frac{5 - 3 + 2(0)}{1} + 1 = 3$$\n",
    "\n",
    "$$D_\\text{out} = K = 1$$\n",
    "\n",
    "Therefore, $x_\\text{out} \\in \\mathbb{R}^{3 \\times 3 \\times 1}$.\n",
    "\n",
    "Further information about specific kernels and a more in-depth introduction to convolution can be found in the `image_processing` repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use activation functions to increase non-linearity in the network. In theory, applying an activation function to convolutional layers is similar to biological action potential thresholding in the firing of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectified Linear Unit  (ReLU)\n",
    "$$f(x) = \\begin{cases} 0 & \\text{for } x \\leq 0 \\\\\n",
    "x & \\text{for } x > 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = keras.layers.Activation('relu')\n",
    "relu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "$$f(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = keras.layers.Activation('sigmoid')\n",
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperbolic Tangent (tanh)\n",
    "$$f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = keras.layers.Activation('tanh')\n",
    "tanh = torch.nn.Tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "$$P(y=j| \\mathbf{x}) = \\frac{e^{\\mathbf{x}^\\mathsf{T}\\mathbf{w}_j}}{\\sum_{k=1}^{k} e^{\\mathbf{x}^\\mathsf{T}\\mathbf{w}_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = keras.layers.Activation('softmax')\n",
    "softmax = torch.nn.Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential Linear Unit (ELU)\n",
    "$$f(\\alpha, x) = \\begin{cases} \\alpha (e^x - 1) & \\text{for } x \\leq 0 \\\\\n",
    "x & \\text{for } x > 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu = keras.layers.Activation('elu')\n",
    "elu = torch.nn.ELU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softplus\n",
    "$$f(x) = \\ln(1 + e^x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = keras.layers.Activation('softplus')\n",
    "softplus = torch.nn.Softplus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softsign\n",
    "$$f(x) = \\frac{x}{1 + |x|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "softsign = keras.layers.Activation('softsign')\n",
    "softsign = torch.nn.Softsign()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Others\n",
    "\n",
    "- Leaky ReLU\n",
    "\n",
    "- PReLU\n",
    "\n",
    "- RReLU\n",
    "\n",
    "- SELU\n",
    "\n",
    "- GELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "#### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling1D, MaxPooling2D, MaxPooling3D\n",
    "from torch.nn import MaxPool1d, MaxPool2d, MaxPool3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D, AveragePooling2D, AveragePooling3D\n",
    "from torch.nn import AvgPool1d, AvgPool2d, AvgPool3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
